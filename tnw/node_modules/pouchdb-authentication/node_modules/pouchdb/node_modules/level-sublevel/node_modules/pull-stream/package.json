{
  "name": "pull-stream",
  "description": "minimal pull stream",
  "version": "2.21.0",
  "homepage": "https://github.com/dominictarr/pull-stream",
  "repository": {
    "type": "git",
    "url": "git://github.com/dominictarr/pull-stream.git"
  },
  "dependencies": {
    "pull-core": "~1.0.0"
  },
  "devDependencies": {
    "tape": "~1.0.0"
  },
  "scripts": {
    "test": "set -e; for t in test/*.js; do node $t; done"
  },
  "author": {
    "name": "Dominic Tarr",
    "email": "dominic.tarr@gmail.com",
    "url": "http://dominictarr.com"
  },
  "license": "MIT",
  "readme": "# pull-stream\n\nMinimal Pipeable Pull-stream\n\nIn [classic-streams](https://github.com/joyent/node/blob/v0.8/doc/api/stream.markdown),\nstreams _push_ data to the next stream in the pipeline.\nIn [new-streams](https://github.com/joyent/node/blob/v0.10/doc/api/stream.markdown),\ndata is pulled out of the source stream, into the destination.\n\n`pull-stream` is a minimal take on pull streams,\noptimized for \"object\" streams, but still supporting text streams.\n\n## Quick Example\n\nStat some files:\n\n```js\npull(\n  pull.values(['file1', 'file2', 'file3']),\n  pull.asyncMap(fs.stat),\n  pull.collect(function (err, array) {\n    console.log(array)\n  })\n)\n```\nnote that `pull(a, b, c)` is basically the same as `a.pipe(b).pipe(c)`.\n\nThe best thing about pull-stream is that it can be completely lazy.\nThis is perfect for async traversals where you might want to stop early.\n\n## Examples\n\nWhat if implementing a stream was this simple:\n\n### Pipeable Streams\n\n`pull.{Source,Through,Sink}` just wrap a function and give it a `.pipe(dest)`!\n\n```js\nvar pull = require('pull-stream')\n\nvar createSourceStream = pull.Source(function () {\n  return function (end, cb) {\n    return cb(end, Math.random())\n  }\n})\n\nvar createThroughStream = pull.Through(function (read) {\n  return function (end, cb) {\n    read(end, cb)\n  }\n})\n\nvar createSinkStream = pull.Sink(function (read) {\n  read(null, function next (end, data) {\n    if(end) return\n    console.log(data)\n    read(null, next)\n  })\n})\n\npull(createSourceStream(), createThroughStream()), createSinkStream())\n```\n\n### Readable & Reader vs. Readable & Writable\n\nInstead of a readable stream, and a writable stream, there is a `readable` stream,\nand a `reader` stream.\n\nSee also:\n* [Sources](https://github.com/dominictarr/pull-stream/blob/master/docs/sources.md)\n* [Throughs](https://github.com/dominictarr/pull-stream/blob/master/docs/throughs.md)\n* [Sinks](https://github.com/dominictarr/pull-stream/blob/master/docs/sinks.md)\n\n### Readable\n\nThe readable stream is just a `function(end, cb)`,\nthat may be called many times,\nand will (asynchronously) `callback(null, data)` once for each call.\n\nThe readable stream eventually `callback(err)` if there was an error, or `callback(true)`\nif the stream has no more data.\n\nif the user passes in `end = true`, then stop getting data from wherever.\n\nAll [Sources](https://github.com/dominictarr/pull-stream/blob/master/docs/sources.md)\nand [Throughs](https://github.com/dominictarr/pull-stream/blob/master/docs/throughs.md)\nare readable streams.\n\n```js\nvar i = 100\nvar randomReadable = pull.Source(function () {\n  return function (end, cb) {\n    if(end) return cb(end)\n    //only read 100 times\n    if(i-- < 0) return cb(true)\n    cb(null, Math.random())\n  }\n})\n```\n\n### Reader (aka, \"writable\")\n\nA `reader`, is just a function that calls a readable,\nuntil it decideds to stop, or the readable `cb(err || true)`\n\nAll [Throughs](https://github.com/dominictarr/pull-stream/blob/master/docs/throughs.md)\nand [Sinks](https://github.com/dominictarr/pull-stream/blob/master/docs/sinks.md)\nare reader streams.\n\n```js\nvar logger = pull.Sink(function (read) {\n  read(null, function next(end, data) {\n    if(end === true) return\n    if(end) throw end\n\n    console.log(data)\n    read(null, next)\n  })\n})\n```\n\nThese can be connected together by passing the `readable` to the `reader`\n\n```js\nlogger()(randomReadable())\n```\n\nOr, if you prefer to read things left-to-right\n\n```js\npull(randomReadable(), logger())\n```\n\n### Through / Duplex\n\nA duplex/through stream is both a `reader` that is also `readable`\n\nA duplex/through stream is just a function that takes a `read` function,\nand returns another `read` function.\n\n```js\nvar map = pull.Through(function (read, map) {\n  //return a readable function!\n  return function (end, cb) {\n    read(end, function (end, data) {\n      cb(end, data != null ? map(data) : null)\n    })\n  }\n})\n```\n\n### Pipeability\n\nEvery pipeline must go from a `source` to a `sink`.\nData will not start moving until the whole thing is connected.\n\n```js\npull(source, through, sink)\n```\n\nsome times, it's simplest to describe a stream in terms of other streams.\npull can detect what sort of stream it starts with (by counting arguments)\nand if you pull together through streams, it gives you a new through stream.\n\n```js\nvar tripleThrough =\n  pull(through1(), through2(), through3())\n//THE THREE THROUGHS BECOME ONE\n\npull(source(), tripleThrough, sink())\n```\n\n## Duplex Streams\n\nDuplex streams, which are used to communicate between two things,\n(i.e. over a network) are a little different. In a duplex stream,\nmessages go both ways, so instead of a single function that represents the stream,\nyou need a pair of streams. `{source: sourceStream, sink: sinkStream}`\n\npipe duplex streams like this:\n\n``` js\nvar a = duplex()\nvar b = duplex()\n\npull(a.source, b.sink)\npull(b.source, a.sink)\n\n//which is the same as\n\nb.sink(a.source); a.sink(b.source)\n\n//but the easiest way is to allow pull to handle this\n\npull(a, b, a)\n\n//\"pull from a to b and then back to a\"\n\n```\n\n## Design Goals & Rationale\n\nThere is a deeper,\n[platonic abstraction](http://en.wikipedia.org/wiki/Platonic_idealism),\nwhere a streams is just an array in time, instead of in space.\nAnd all the various streaming \"abstractions\" are just crude implementations\nof this abstract idea.\n\n[classic-streams](https://github.com/joyent/node/blob/v0.8.16/doc/api/stream.markdown),\n[new-streams](https://github.com/joyent/node/blob/v0.10/doc/api/stream.markdown),\n[reducers](https://github.com/Gozala/reducers)\n\nThe objective here is to find a simple realization of the best features of the above.\n\n### Type Agnostic\n\nA stream abstraction should be able to handle both streams of text and streams\nof objects.\n\n### A pipeline is also a stream.\n\nSomething like this should work: `a.pipe(x.pipe(y).pipe(z)).pipe(b)`\nthis makes it possible to write a custom stream simply by\ncombining a few available streams.\n\n### Propagate End/Error conditions.\n\nIf a stream ends in an unexpected way (error),\nthen other streams in the pipeline should be notified.\n(this is a problem in node streams - when an error occurs,\nthe stream is disconnected, and the user must handle that specially)\n\nAlso, the stream should be able to be ended from either end.\n\n### Transparent Backpressure & Laziness\n\nVery simple transform streams must be able to transfer back pressure\ninstantly.\n\nThis is a problem in node streams, pause is only transfered on write, so\non a long chain (`a.pipe(b).pipe(c)`), if `c` pauses, `b` will have to write to it\nto pause, and then `a` will have to write to `b` to pause.\nIf `b` only transforms `a`'s output, then `a` will have to write to `b` twice to\nfind out that `c` is paused.\n\n[reducers](https://github.com/Gozala/reducers) reducers has an interesting method,\nwhere synchronous tranformations propagate back pressure instantly!\n\nThis means you can have two \"smart\" streams doing io at the ends, and lots of dumb\nstreams in the middle, and back pressure will work perfectly, as if the dumb streams\nare not there.\n\nThis makes laziness work right.\n\n## License\n\nMIT\n",
  "readmeFilename": "README.md",
  "bugs": {
    "url": "https://github.com/dominictarr/pull-stream/issues"
  },
  "_id": "pull-stream@2.21.0",
  "_from": "pull-stream@~2.21.0"
}
